{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import basename\n",
    "import collections\n",
    "import tflearn\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tflearn.data_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def check_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns whether it's empty or not\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    if(len(filenames)==0):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(label2id, label):\n",
    "    \"\"\"\n",
    "    Returns label for a folder\n",
    "    \"\"\"\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "    return X\n",
    "\n",
    "def get_image_map(folder, label):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    image_map = {}\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        #img_arr = get_img_array(f) / 255.0\n",
    "        #img_arr = get_img_array(f)\n",
    "        #img_arr = img_arr.flatten() / 255.0\n",
    "        img_arr = np.load(f)\n",
    "        img_arr = img_arr.flatten()\n",
    "        name = basename(f)\n",
    "        #print(name)\n",
    "        num = int(name.split(\"_\")[1].split(\".\")[0])\n",
    "        image_map[num] = [img_arr, label]\n",
    "        #images.append(img_arr)\n",
    "    #X = np.column_stack(images)\n",
    "    return image_map\n",
    "\n",
    "def get_train_data(data_root_path, label_mapping_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    labels = os.listdir(data_root_path)\n",
    "    id2label, label2id = get_label_mapping(label_mapping_path)\n",
    "    print(label2id)\n",
    "    image_map = {}\n",
    "    for label in labels:\n",
    "        train_data_path = data_root_path + label\n",
    "        if(check_files(train_data_path)) :\n",
    "            temp_map = get_image_map(train_data_path, label)\n",
    "            image_map.update(temp_map)\n",
    "    new_map = collections.OrderedDict(sorted(image_map.items()))\n",
    "    for k, v in new_map.items():\n",
    "        X.append(v[0])\n",
    "        y.append(get_label(label2id, v[1]))\n",
    "    X = np.array(X)\n",
    "    return X, np.array(y)\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'D': 1, 'W': 2}\n",
      "{'A': 0, 'D': 1, 'W': 2}\n",
      "(3728, 128)\n",
      "(3728,)\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "# The data folder can have any number of subfolders. \n",
    "#The name of the subfolder will be treated as the label for all the images in that subfolder.\n",
    "#Add all the subfolder names in labels.txt\n",
    "data_root_path = './middle_data/last_layer3/' #will take all the folders in this directory to be used as labels\n",
    "data_root_path1 = './middle_data/last_layer4/'\n",
    "label_mapping_path = './labels.txt' #labels.txt should NOT be in the data_root_path\n",
    "X_from_image, y_from_image = get_train_data(data_root_path, label_mapping_path) # this may take a few minutes\n",
    "X_from_image1, y_from_image1 = get_train_data(data_root_path1, label_mapping_path)\n",
    "    \n",
    "X_from_image = np.concatenate((X_from_image, X_from_image1), axis=0)\n",
    "y_from_image = np.concatenate((y_from_image, y_from_image1), axis=0)\n",
    "print(X_from_image.shape)\n",
    "#print(X_from_image)\n",
    "print(y_from_image.shape)\n",
    "#print(y_from_image)\n",
    "print('Data loading done')\n",
    "#print(type(X_from_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(X_from_image, y_from_image, num_frames, num_classes, input_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    #print(X_from_image.shape)\n",
    "    #print(\"hi\")\n",
    "    image_seq = deque()\n",
    "    #y_list = deque()\n",
    "    for row in range(0,len(X_from_image)):\n",
    "        image_seq.append(X_from_image[row])\n",
    "        #y_list.append(y_from_image[row])\n",
    "        if len(image_seq) == num_frames:\n",
    "            X.append(np.array(list(image_seq)))\n",
    "            y.append(y_from_image[row])\n",
    "            #y.append(np.array(list(y_list)))\n",
    "            image_seq.popleft()\n",
    "            #y_list.popleft()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(X.shape)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    print(y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn(images, input_size, num_classes):\n",
    "    \n",
    "    rnn = tflearn.input_data(shape=[None, images, input_size])\n",
    "    rnn = tflearn.lstm(rnn, 512, dropout=0.8, return_seq=True)\n",
    "    rnn = tflearn.lstm(rnn, 512)\n",
    "    rnn = tflearn.fully_connected(rnn, num_classes, activation='softmax')\n",
    "    rnn = tflearn.regression(rnn, optimizer='adam',\n",
    "                             loss='categorical_crossentropy', name=\"output1\")\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, seq_length, batch_size, save_path):\n",
    "    \n",
    "    \n",
    "    input_length = X_train.shape[2]\n",
    "    num_classes = len(y_train[0])\n",
    "    \n",
    "    rnn = get_rnn(seq_length, input_length, num_classes)\n",
    "\n",
    "    model = tflearn.DNN(rnn, tensorboard_verbose=1)\n",
    "    model.fit(X_train, y_train, validation_set=(X_test, y_test),\n",
    "              show_metric=True, batch_size=batch_size, snapshot_step=100,\n",
    "              n_epoch=5)\n",
    "    #print(model.session)\n",
    "    #builder = tf.saved_model.builder.SavedModelBuilder('./modelPath1')\n",
    "    #builder.add_meta_graph_and_variables(model.session, [tf.saved_model.tag_constants.TRAINING],\n",
    "                                         #signature_def_map=None, assets_collection=None)\n",
    "    model.save(save_path)\n",
    "    \n",
    "    #test to predict lable for just one image\n",
    "    #t = []\n",
    "    #t.append(X_train[0])\n",
    "    #print(model.predict_label(t))\n",
    "    results =  model.predict_label(X_test)\n",
    "    \n",
    "    #builder.save()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.33049\u001b[0m\u001b[0m | time: 9.042s\n",
      "| Adam | epoch: 005 | loss: 0.33049 - acc: 0.8727 -- iter: 3328/3355\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.33151\u001b[0m\u001b[0m | time: 10.220s\n",
      "| Adam | epoch: 005 | loss: 0.33151 - acc: 0.8682 | val_loss: 0.25286 - val_acc: 0.9437 -- iter: 3355/3355\n",
      "--\n",
      "INFO:tensorflow:/Users/zhengyixing/Desktop/final_project/model_file/checkpoints/RNN_data3_4/RNN_data3_4_hsv is not in all_model_checkpoint_paths. Manually adding it.\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "[ 0.80851064  0.86792453  0.98168498]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #filename = 'data/cnn-features-frames-2.pkl'\n",
    "    input_length = 128 #look at the data and figure this out!\n",
    "    seq_length = 1\n",
    "    batch_size = 64\n",
    "    num_classes = 3\n",
    "    \n",
    "    data_root_path = './middle_data/last_layer3_hsv/' #will take all the folders in this directory to be used as labels\n",
    "    data_root_path1 = './middle_data/last_layer4_hsv/'\n",
    "    label_mapping_path = './labels.txt' #labels.txt should NOT be in the data_root_path\n",
    "    save_path = './model_file/checkpoints/RNN_data3_4/RNN_data3_4_hsv'\n",
    "    X_from_image, y_from_image = get_train_data(data_root_path, label_mapping_path)\n",
    "    X_from_image1, y_from_image1 = get_train_data(data_root_path1, label_mapping_path)\n",
    "    \n",
    "    X_from_image = np.concatenate((X_from_image, X_from_image1), axis=0)\n",
    "    y_from_image = np.concatenate((y_from_image, y_from_image1), axis=0)\n",
    "    \n",
    "    X_test = []\n",
    "    y_test =[]\n",
    "    X_train, X_test, y_train, y_test = get_data(X_from_image, y_from_image, seq_length, num_classes, input_length)\n",
    "    \n",
    "    results = train(X_train, y_train, X_test, y_test, seq_length, batch_size, save_path)\n",
    "    \n",
    "    # predict the accuracy for each class\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = results[:,0]\n",
    "    cmat = confusion_matrix(y_true, y_pred)\n",
    "    print(cmat.diagonal()/cmat.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373, 3)\n",
      "373\n",
      "373\n",
      "(3355, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)\n",
    "results = np.array(results)\n",
    "print(len(results[:,0]))\n",
    "print(len(y_test))\n",
    "#print( results[:,0])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80851064,  0.86792453,  0.98168498])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = results[:,0]\n",
    "cmat = confusion_matrix(y_true, y_pred)\n",
    "cmat\n",
    "cmat.diagonal()/cmat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(y_true)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.8085106382978723\n",
      "1\n",
      "0.8679245283018868\n",
      "2\n",
      "0.9816849816849816\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, 3):\n",
    "    count = 0\n",
    "    counti = 0\n",
    "    for i in range(0, len(y_true)):\n",
    "        if(y_true[i]==k):\n",
    "            counti=counti+1\n",
    "            if(y_pred[i]==k):\n",
    "                count = count+1\n",
    "    print(k)\n",
    "    print(count/counti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
